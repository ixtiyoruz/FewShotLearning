{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototypical Networks for Few-shot Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the idea is to solve the problem of few shot learning in terms of classifier.\n",
    "- Classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. \n",
    "- So, we don't give the model totally new dataset it will be given only a small number of them.\n",
    "Humans are well suited at this task, given one image we can learn what it is and classify the object.\n",
    "Prototypical networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class.\n",
    "- What is prototype representation of a class ?\n",
    "    - I bet this paper is all about this, we have to read more \n",
    "- Their assumptions:\n",
    "    - Our approach, prototypical networks, is based on the idea that there exists an embedding in which points cluster around a single prototype representation for each class.\n",
    "- They learn an embedding of the meta-data into a shared space to serve as the prototype for each class.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, sys\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path, use_pillow=False, height=256):\n",
    "    \"\"\"\n",
    "    reads and returns image\n",
    "    Arguments:\n",
    "        path: str - path to the image\n",
    "    Returns \n",
    "        img: numpy array        \n",
    "    \"\"\"\n",
    "    if(use_pillow):\n",
    "        img = Image.open(path)\n",
    "    else:\n",
    "        img = cv2.resize(cv2.imread(path, 1), (height, height))\n",
    "    return img\n",
    "\n",
    "def load_all_images(paths):\n",
    "    data = []\n",
    "    for idx, path in enumerate(paths):\n",
    "        data.append(read_image(path))\n",
    "        if(idx % 1000 == 0):\n",
    "            print(idx)\n",
    "            print(path)\n",
    "    return data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Birds Dataset Meta should return two subset of dataset:\n",
    "1. Support set which is used to get the prototype representations of the classes.\n",
    "2. Query set which is used to train the model.\n",
    "Open questions :\n",
    "1. Do they have the same classes(classes in the support set are the same with the classes with the query set) ?\n",
    "    - yes they should be the same \n",
    "2. Is it ok if support set and query set have the same images ?\n",
    "    - it is not recommended, or must not be true\n",
    "\"\"\"\n",
    "def start_process():\n",
    "    print('Starting', mp.current_process().name)\n",
    "class BirdsDatasetMeta(Dataset):\n",
    "    \"\"\"\n",
    "    this is a Birds dataset reader for meta learning\n",
    "    \"\"\"\n",
    "    def __init__(self, images_pd, imgs,classes, transform = None, n_class_per_batch=5, n_support= 10, n_query=10):\n",
    "        self.n_class_per_batch = n_class_per_batch\n",
    "        self.n_support = n_support\n",
    "        self.n_query = n_query\n",
    "        self.images_pd = images_pd\n",
    "        self.imgs = imgs\n",
    "        self.classes = classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.classes) # each class is one sample\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "       # so we have the same type of classes for K shot learning.\n",
    "        chosen_classes = np.random.choice(self.classes, self.n_class_per_batch, replace=False)\n",
    "        feats = []\n",
    "        labels = []\n",
    "        for c in chosen_classes:\n",
    "            datax_cls = self.images_pd[self.images_pd[\"class_id\"]==c]            \n",
    "            perm = np.random.permutation(datax_cls[\"index\"].values)\n",
    "            if(self.imgs is not None):\n",
    "                sample_cls = self.imgs[perm[:(self.n_support+self.n_query)]]\n",
    "            else:\n",
    "                raise(\"not implemented error\")\n",
    "            feats.append(sample_cls)\n",
    "            labels.append([c] * len(sample_cls))\n",
    "        return np.float32(feats), np.int32(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "./CUB_200_2011\\images\\001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n",
      "1000\n",
      "./CUB_200_2011\\images\\019.Gray_Catbird/Gray_Catbird_0039_21040.jpg\n",
      "2000\n",
      "./CUB_200_2011\\images\\036.Northern_Flicker/Northern_Flicker_0037_28751.jpg\n",
      "3000\n",
      "./CUB_200_2011\\images\\052.Pied_billed_Grebe/Pied_Billed_Grebe_0091_35276.jpg\n",
      "4000\n",
      "./CUB_200_2011\\images\\069.Rufous_Hummingbird/Rufous_Hummingbird_0046_59647.jpg\n",
      "5000\n",
      "./CUB_200_2011\\images\\086.Pacific_Loon/Pacific_Loon_0022_75405.jpg\n",
      "6000\n",
      "./CUB_200_2011\\images\\103.Sayornis/Sayornis_0079_98434.jpg\n",
      "7000\n",
      "./CUB_200_2011\\images\\120.Fox_Sparrow/Fox_Sparrow_0113_114389.jpg\n",
      "8000\n",
      "./CUB_200_2011\\images\\137.Cliff_Swallow/Cliff_Swallow_0005_133696.jpg\n",
      "9000\n",
      "./CUB_200_2011\\images\\154.Red_eyed_Vireo/Red_Eyed_Vireo_0036_156727.jpg\n",
      "10000\n",
      "./CUB_200_2011\\images\\170.Mourning_Warbler/Mourning_Warbler_0078_795377.jpg\n",
      "11000\n",
      "./CUB_200_2011\\images\\187.American_Three_toed_Woodpecker/American_Three_Toed_Woodpecker_0030_796144.jpg\n"
     ]
    }
   ],
   "source": [
    "ROOT = \"./CUB_200_2011\"\n",
    "images_path_file = os.path.join(ROOT, \"images.txt\")\n",
    "images_class_file = os.path.join(ROOT, \"image_class_labels.txt\")\n",
    "with open(images_path_file, 'r') as f:\n",
    "    images = f.readlines()\n",
    "images = [im_path.split(\" \") for im_path in images]\n",
    "images = [[int(im_path[0]), int(im_path[-1][:3]), os.path.join(ROOT, \"images\",im_path[-1][:-1])] for im_path in images]\n",
    "images_pd = pd.DataFrame(images, columns =[\"index\", \"class_id\", \"path\"])\n",
    "imgs= load_all_images(images_pd[\"path\"].values)\n",
    "classes = images_pd[\"class_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imgs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e34295f143b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mimgsn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBirdsDatasetMeta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages_pd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'imgs' is not defined"
     ]
    }
   ],
   "source": [
    "imgsn = np.float32(imgs)\n",
    "del imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = BirdsDatasetMeta(images_pd, imgsn, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ROOT, \"classes.txt\"), \"r\") as f:\n",
    "    class_info = f.readlines()\n",
    "# to print the name of the class:\n",
    "# class_info[label-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 20)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the model is composed of four convolutional blocks. Each block comprises a 64-filter 3 Ã— 3 convolution, batch normalization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrototypeModel(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        nn.Module.__init__(self)        \n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            self.block(in_dim, hidden_dim),\n",
    "            self.block(hidden_dim, hidden_dim),\n",
    "            self.block(hidden_dim, hidden_dim),\n",
    "            self.block(hidden_dim, out_dim),\n",
    "        )\n",
    "    def block(self,in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        x= self.conv_blocks(x)\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PrototypeModel(3, 256, 1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.zeros((1, 3,256,256))\n",
    "ret = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 409600])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
