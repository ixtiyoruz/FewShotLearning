{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototypical Networks for Few-shot Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the idea is to solve the problem of few shot learning in terms of classifier.\n",
    "- Classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. \n",
    "- So, we don't give the model totally new dataset it will be given only a small number of them.\n",
    "Humans are well suited at this task, given one image we can learn what it is and classify the object.\n",
    "Prototypical networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class.\n",
    "- What is prototype representation of a class ?\n",
    "    - I bet this paper is all about this, we have to read more \n",
    "- Their assumptions:\n",
    "    - Our approach, prototypical networks, is based on the idea that there exists an embedding in which points cluster around a single prototype representation for each class.\n",
    "- They learn an embedding of the meta-data into a shared space to serve as the prototype for each class.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, sys\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "from threading import Thread\n",
    "from scipy import ndimage\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#Check GPU support, please do activate GPU\n",
    "print(torch.cuda.is_available())\n",
    "ctx = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 28\n",
    "def read_characters(alphabet_path):\n",
    "    character_folders = os.listdir(alphabet_path)\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for character_folder in character_folders:\n",
    "        im_names = os.listdir(os.path.join(alphabet_path, character_folder))\n",
    "        for img_name in im_names:\n",
    "            img = cv2.resize(cv2.imread(os.path.join(alphabet_path, character_folder, img_name)), (WIDTH, WIDTH))\n",
    "            #rotations of image\n",
    "            rotated_90 = ndimage.rotate(img, 90)\n",
    "            rotated_180 = ndimage.rotate(img, 180)\n",
    "            rotated_270 = ndimage.rotate(img, 270)\n",
    "            imgs.extend((img, rotated_90, rotated_180, rotated_270))\n",
    "            labels.extend((\n",
    "                alphabet_path + '_' + character_folder + '_0',\n",
    "                alphabet_path + '_' + character_folder + '_90',\n",
    "                alphabet_path + '_' + character_folder + '_180',\n",
    "                alphabet_path + '_' + character_folder + '_270'\n",
    "            ))\n",
    "            \n",
    "    return np.array(imgs), np.array(labels)\n",
    "            \n",
    "def read_alphabets(folder):\n",
    "    alphabets = os.listdir(folder)\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    \n",
    "    for alphabet in alphabets:\n",
    "        alph_imgs, alph_labels = read_characters(os.path.join(folder, alphabet))\n",
    "        imgs.append(alph_imgs)\n",
    "        labels.append(alph_labels)\n",
    "    imgs = np.concatenate(imgs, 0)\n",
    "    labels = np.concatenate(labels, 0)\n",
    "    return imgs, labels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainx, trainy = read_alphabets(\"images_background\")\n",
    "testx, testy = read_alphabets(\"images_evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(trainy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data:\n",
    "- In few shot classification, we have:\n",
    "    - N labelled examples S = {(x1, y1), . . . ,(xN , yN )}.\n",
    "        - Each x is D dimensional feature vector.\n",
    "        - y is the corresponding label\n",
    "        - Sk denotes set of examples labelled with class k.\n",
    "\n",
    "# The model:\n",
    "- Prototypical networks compute M dimensional representation ck âˆˆ RM, \n",
    "- So the model can be defined like this:   f(R_D ->R_M)              which converts D dimensional data into M dimension.\n",
    "- So, firstly we will compute prototype representations of each class and compute the embedding of query point x.\n",
    "- Then it will compute the probability based on softmax, given distance function d.\n",
    "\n",
    "\n",
    "paper is available here:\n",
    "https://arxiv.org/pdf/1703.05175.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "class Alphabet(Dataset):\n",
    "    def __init__(self, imgs, labels, classes, my_transforms, n_class_per_batch=60, n_support=5, n_batch=5, epoch_size=2000, test=False):\n",
    "        Dataset.__init__(self)\n",
    "        self.imgs = imgs\n",
    "        self.indices = np.arange(len(self.imgs))\n",
    "        self.labels = labels\n",
    "        self.n_class_per_batch = n_class_per_batch\n",
    "        self.n_support = n_support\n",
    "        self.classes = classes\n",
    "        self.n_batch = n_batch\n",
    "        self.epoch_size = epoch_size\n",
    "        self.my_transforms = my_transforms\n",
    "        self.curr_index = 0\n",
    "        self.test = test\n",
    "    def __len__(self):\n",
    "        return self.epoch_size\n",
    "    def __getitem__(self, ind):\n",
    "        \"\"\"\n",
    "        first of all we need to generate random classes \n",
    "        to sample from\n",
    "        then get random unique images for both support and training.\n",
    "        \"\"\"\n",
    "        if(self.test):\n",
    "            curr_classes = self.classes[self.curr_index:self.curr_index+self.n_class_per_batch]\n",
    "            self.curr_index += self.n_class_per_batch\n",
    "        else:\n",
    "            curr_classes = np.random.choice(self.classes, self.n_class_per_batch, replace=False)\n",
    "            \n",
    "        support = []\n",
    "        batch = []\n",
    "        for cl in curr_classes:\n",
    "            indices = np.where(self.labels == cl)[0]\n",
    "            \n",
    "            sel_img_indices = np.random.choice(indices, self.n_support + self.n_batch, replace=True)\n",
    "            sel_imgs = self.imgs[sel_img_indices] \n",
    "            converted_imgs = []\n",
    "            for img in sel_imgs:\n",
    "                imgt = self.my_transforms(img)\n",
    "                converted_imgs.append(self.my_transforms(img))\n",
    "            converted_imgs = torch.stack(converted_imgs, 0)\n",
    "            support.append(converted_imgs[:self.n_support])\n",
    "            batch.append(converted_imgs[self.n_support:])\n",
    "        support = torch.stack(support, 0).float()\n",
    "        batch = torch.stack(batch, 0).float()\n",
    "        # we dont need classes we are sure that the each element in the batch is belong to one class\n",
    "        return support, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "#             transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "                                           ]\n",
    "        )\n",
    "test_transforms = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "#             transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "                                           ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Alphabet(trainx, trainy, classes, n_class_per_batch=5, my_transforms=test_transforms)\n",
    "# s_in, b_in = data[0]\n",
    "# display_sample(s_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrototypeModel(nn.Module):\n",
    "    def __init__(self,in_dim,hid_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            self.block(3, hid_dim),\n",
    "            self.block(hid_dim, hid_dim),\n",
    "            self.block(hid_dim, hid_dim),\n",
    "            self.block(hid_dim, out_dim)\n",
    "        )\n",
    "    def block(self,in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "    def forward(self, x):\n",
    "        x = self.conv_blocks(x)\n",
    "        return x.reshape(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(x, y):\n",
    "    \"\"\"\n",
    "    Computes euclidean distance btw x and y\n",
    "    Args:\n",
    "      x (torch.Tensor): shape (n, d). n usually n_way*n_query\n",
    "      y (torch.Tensor): shape (m, d). m usually n_way\n",
    "    Returns:\n",
    "      torch.Tensor: shape(n, m). For each query, the distances to each centroid\n",
    "    \"\"\"\n",
    "    n = x.size(0)\n",
    "    m = y.size(0)\n",
    "    d = x.size(1)\n",
    "    assert d == y.size(1)\n",
    "\n",
    "    x = x.unsqueeze(1).expand(n, m, d)\n",
    "    y = y.unsqueeze(0).expand(n, m, d)\n",
    "\n",
    "    return torch.pow(x - y, 2).sum(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(model, support_in, batch_in):\n",
    "    \"\"\"\n",
    "    model: PrototypeModel - the model to be used to get the predictions\n",
    "    support_in: tensor with size n_class_per_batch x n_support x w x h x 3 \n",
    "    - input data  of support dataset \n",
    "    batch_in: tensor with shape n_class_per_batch x n_batch x w x h x 3\n",
    "    - input data  of batch dataset \n",
    "    \"\"\" \n",
    "    n_class = batch_in.shape[0]\n",
    "    n_batch = batch_in.shape[1]\n",
    "    n_supoort = support_in.shape[1]\n",
    "    \n",
    "    # change the size to suitable format\n",
    "    support_in = support_in.contiguous().view(n_class * n_supoort, *support_in.shape[-3:])\n",
    "    batch_in = batch_in.contiguous().view(n_class * n_batch, *batch_in.shape[-3:])\n",
    "    s_pred = model(support_in)\n",
    "    b_pred = model(batch_in)\n",
    "    z_dim = b_pred.size(-1)\n",
    "    \n",
    "    labels = torch.arange(0, n_class).view(n_class,1, 1).expand(n_class, n_batch, -1).long()\n",
    "    labels = Variable(labels, requires_grad=False)\n",
    "    labels = labels.to(ctx)\n",
    "    \n",
    "    # so s_pred will play the role of class prototypes so we have to take the mean of each class\n",
    "    s_pred = s_pred.view(n_class, n_supoort, z_dim).mean(1)\n",
    "    b_pred = b_pred\n",
    "    # this computes distances of each batch sample from all the prototype means, \n",
    "    # so the output can be shaped to n_class x n_batch x n_classs which means n_class \n",
    "    dists = torch.cdist(b_pred, s_pred)\n",
    "    log_p_y = torch.log_softmax(-dists, dim = 1).view(n_class, n_batch, -1)\n",
    "    \n",
    "    # what gather does is selects specific element from given index and the dimension of the \n",
    "    # indexing should be the same with the array\n",
    "    loss = -log_p_y.gather(2, labels).mean()\n",
    "    _, y_hat = log_p_y.max(2)\n",
    "    acc_val = torch.eq(y_hat, labels.squeeze()).float().mean()\n",
    "    return loss, acc_val, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the get_loss function\n",
    "# s_in, b_in = data[0]\n",
    "# display_sample(s_in)\n",
    "# print(s_in.shape, b_in.shape)\n",
    "# get_loss(model, s_in.to(ctx), b_in.to(ctx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_sample(sample):\n",
    "    \"\"\"\n",
    "    Displays sample in a grid\n",
    "    Args:\n",
    "      sample (torch.Tensor): sample of images to display\n",
    "    \"\"\"\n",
    "    print(sample.shape)\n",
    "    #need 4D tensor to create grid, currently 5D\n",
    "    sample_4D = sample.view(sample.shape[0]*sample.shape[1], *sample.shape[2:])\n",
    "    #make a grid\n",
    "    out = torchvision.utils.make_grid(sample_4D, nrow=sample.shape[1])\n",
    "    plt.figure(figsize = (16,7))\n",
    "    plt.imshow(out.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5, 3, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaUAAAGeCAYAAAA0bx7AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3df4xdZ33n8c93YzdtsBEk44Q0490Z6hFtiNoFRUCLvbI2ZUkpSpBiI6NFsg12tBK7BdZ2STbSsiuEtpLtUqRdimyDZ3aLgNihJEKiJXIXsFH44cACgRA8xW48xMSehd3a7Ypmynf/mPvceex55t5z7z333uec+35Jls+ce+85z3nm3HnO93ue5znm7gIAIAf/ZNgFAAAgoFECAGSDRgkAkA0aJQBANmiUAADZoFECAGSjb42Smd1tZs+Y2ayZPdCv/QAA6sP6MU7JzK6T9ENJb5A0J+kbkt7m7t8vfWcAgNpY1aftvkbSrLv/SJLM7FOS7pWUbJTMjBG8ADA65t19XeqFfqXvbpN0Pvp5rrEOAIC/WemFfkVKllh3VTRkZvdLur9P+wcAVFC/GqU5Seujn8clPRe/wd0PSTokkb4DACzqV/ruG5KmzGzSzH5J0jZJj/VpXwCAmuhLpOTuC2b2byX9paTrJH3c3b/X4Tb6UbTKM1ueGaWu0qirzlBfxVFXxaXqquX7c6jIVPouh3LliC9DcdRVZ6iv4qir4lZolJ509ztTLzCjAwAgGzRKAIBs0CgBALJBowQAyAaNEgAgGzRKAIBs0CgBALJBowQAyEa/5r6rhQMHDkiS9u3bt+y1kydPNpc3bty47PVNmzZJkk6dOrXstbNnzzaXJyYmei0mANQGkRIAIBs0SgCAbNRq7rtz585JkiYnJ1u+r9W2p6enm8s7d+6UlE6xhX2ttL0w39OaNWua665cudJRWVKYc6s46qoz1Fdx1FVxzH0HAKisWkVKqRY5dCqIo6dW2463MT4+Lkk6f/58y/e1ipS2b9/eXDczM7PsfURK/UNddYb6Ki73ukpli+IOVoNEpAQAqCwaJQBANmo5TqmsMHrVqt6rp9OnLlZV6jh37NghSTp69GihbaQ6maQcPny4ubxr165iBayxrVu3NpePHz8uaanupeL1j+o4cuRIc3n37t2FPhO+o/Hfx3Xr1kmS5ufnl70/7qR1+fLlrsrZDSIlAEA2atnRoVXHg3bbjt8XuoKnbhCmtheuUqWlq9d4X2XcHO1mG2FWiQcffLC5Lp6RolutoiNpKfIpeozx9vbs2SNpaVYNaSl6iiOqor/LIu9fSauZPbrZXhgasHbt2o4/G0eGIWKMjzPUf1xHRSPWHG/ex8MoWtVXqpxxtN3pudjOsOsq9Tsv+vuNh7iEv22pv2dlHSMdHQAAlUWjBADIBum7Fu/rNH0XrwuhdJzOGlb6LvWZVmOwuilL0bFacVqp0+2l3hfKH45npfcV2a60dMM33ACOxWmPMKNH0WNLlSsu84c+9KHm8pYtW5Z9JjXBb+q8S60LZWxXvn6dn2VtL3XOpm7Uh33E370wRjCVuupGL3XVqo7abSOMP2o3o0yr/RZN38X7CPslfQcAGCm16hKeQ9SXs/379zeXUzftyxau0OOZLFJX693c8A9CBFHWaPW9e/cuW9cu8i5DKjqKhY4puQ4xSP1eU7Op9CLVOSec062GEMTiq/9BSv3e4uMJ53E7ofyD6Oafyj4MApESACAbNEoAgGzUKn03LEVTKnH6bBji1FRI38WzARw7dqzU/RWtlzAWpYyxU70K44A2b95c6P3dpNNSaa3UuKJYKq3YaceKQUs98iWk2cpKP4W6SqXvck11pjoStJKabSF1jpStjBltukGkBADIRq26hLfZR6FtF+0SPiy9dAnv5+wSReu0VaeBbrrUFu22X6Sc7cTdxMPVay/bi6OfgwcPNpdb1VEcYaQeQpnq4jvsGR3i7W7cuFFS8ag4/mw4tlQEVvQci7XaXiflarXfVuUr2uW67K7ZRbuEpz4bn0NFozW6hAMAKotGCQCQjZHs6DDsyRQHoayxISmDuMmak36dL/Fks3H6rpXUDf126eVhn9vtnr48LL2kwnoRzqeitwXK/i7ndDsihUgJAJCNkYmU4quhcOMwvgLJtftot+IbyQsLC8tef+GFFyRJq1evbq4Lj95oN7vAI4880nW5Qp0Pa7R4ToqO4pd6u7qNI7JhiLuuDyJSCt/vVAeGbmZRKEP89ydkGvqZzagyIiUAQDZolAAA2RiZ9F0shPVxSB0eCRCH9MOegaEX7dJjqdHa8VM+W+l0BobU2Jp47MWZM2cKbWdYUjMwhPMljLvpRDj2+DEU7eqgm7E0wdjYWNefraLwe9qwYcOy13JIG4d0ZjepzGF3UhhEJyciJQBANkYyUkoJV7zD7j47aHv27Gkuhygm1e04NRtA0Sghvrr60pe+tOL2ujGIRxGEKCWuq9RN8lbH9JWvfKW5fOTIEUlXdyhJXdV3I3RWyV27GQRmZ2c72l5cl6EO4m3k+L2Oo55BdnqI9zWsR3m0QqQEAMgGjRIAIBtdT8hqZusl/XdJL5P0C0mH3P3DZnajpE9LmpB0TtJb3f1nbbbV9wlZ62LQs1Gk9hcecdFuPFO/DGtC1liYTDW+WZ16xEBKP39fITUTp2W6mei20892qui4wNTkoWWXoZfJTWO91FWr+og7JYXxhd0Iac34cTVBN5M6FzXICVkXJO1x99+Q9DpJ7zKz2yU9IOmEu09JOtH4GQCAtkp7dIWZPSrpvzb+bXb3C2Z2q6Qvuvsr2nyWSKmgUZi3r524Ds6fPy8p3dV3lOsq7t6/Zs2aQp8Z5frq1CjU1fr165vLc3NzkvKPlOKdTkh6laSvSbrF3S9IUuP/m8vYBwCg/nruEm5mayQ9Iuk97v63RXPFZna/pPt73T8AoD56apTMbLUWG6RPuPtnGqufN7Nbo/TdxdRn3f2QpEON7dQr7sXA5DBCP0dFU3bASp5++unm8kc/+tGB7bfr9J0thkQfk/S0u/9x9NJjksIDVLZLerT74gEARkkvXcI3Sjop6bta7BIuSf9Bi/eVHpb0TyU9K2mru/+0zbbo6FDQKNxgbafdbACp9xV5/6ijvoqjrorrtKND1+k7dz8laaUbSHd1u10AwOhiRgcAQDZolAAA2aBRAgBkg0dXoHK4oQzUF5ESACAbNEoAgGzQKAEAskGjBADIBo0SACAbNEoAgGzQKAEAskGjBADIBo0SACAbNEoAgGzQKAEAskGjBADIBo0SACAbNEoAgGxk++iKFZ7rjgTqqjjqqjPUV3HUVTmIlAAA2aBRAgBkg0YJAJANGiUAQDay7ejg7sMuQpZSN1OpqzTqqjPUV3HUVXGddgAhUgIAZINGCQCQDRolAEA2aJQAANmgUQIAZINGCQCQDRolAEA2aJQAANmgUQIAZINGCQCQDRolAEA2aJQAANmgUQIAZINGCQCQjWwfXVEXk5OTzeWzZ88OsSQAkD8iJQBANmiUAADZ6Dl9Z2bXSTot6cfu/mYzu1HSpyVNSDon6a3u/rNe91M1qacthnVbtmxprjt27NjAylQlPNkTGE1lRErvlvR09PMDkk64+5SkE42fAQBoq6dIyczGJf2+pA9K+veN1fdK2txYnpH0RUnv62U/VbZjx47mcrjSn5mZGVJpACBvvUZKfyLpDyX9Ilp3i7tfkKTG/zf3uA8AwIjoulEyszdLuujuT3b5+fvN7LSZne62DACAeuklffd6SfeY2Zsk/bKkF5vZn0l63sxudfcLZnarpIupD7v7IUmHJMnMhn4HO3VjfXx8XJJ0/vz5UvYxPT191f85OHfuXHN5YmJiaOUAAKmHSMndH3T3cXefkLRN0l+5+9slPSZpe+Nt2yU92nMpAQAjoR8zOvyRpIfN7J2SnpW0tQ/76Mnc3Jwkaf369YXeF0dRdeuWHM84cfToUUlXd85A/uLzOJyzdTtPMTpKaZTc/Yta7GUnd//fku4qY7sAgNHCjA4AgGyM5ISsqbRdKt1x4MABSdK+ffv6XqZhiTs37Ny5UxLpu6oIHWZCyk5aSsFWUTj/pKVjW7NmTXPd5cuXu952SMHv2bOnuS58vyFNTU01l9/3vsVhpbt27RpKWYiUAADZsBxuiKa6hBctV9y9uugVftF51UJ36bgzQNFyhX3EZSrjKrafc8KFbfeyvfj3EV/5Bp3WXzefLXMb/dTJ0IBW53ZZHXGGXV/x/sPxpuqomzKFbceZgV4eJTPsuipb6nhiIUqNI9cet/2ku9+ZeoFICQCQDRolAEA2Kp++i0PD1AwM8YwF14rTcqn99pK+W7t2rSTpypUrHX+2lUGk7y5dutRcNzY21nW5Qqok9TtoV+Y6p+/apUpa2b9/f3N57969K26vlxRXr9tptd3w2JbUI1vavW/16tWSpIWFhY7LR/qutXAuSdLBgwclLf09laQnnnhi2bqiSN8BACqr8pFSHImE6KQb4Qo0vmJIRUq9qEqk1E3njKKdJML74mg2dfU1CpFSfKXebt7BcA7GUWc4lnhdeF8cYcQPlSxSrlgZ9ZXq6p3abvz9Dd/rjRs3NtedPHlyWTnDjfd23cXDfuOyVLlTSDupqLJoXQ0iqhSREgCgCmiUAADZqHz6LhZC/lQar+zjLDrGpKx0QdDPtEEoa3xsZY8rKvt9ZZSpqHXr1jWX5+fnu95eKFecVkvd+G/12U4McmxYN/toJZWGPHXqVHPdpk2bui7LsNN3/RxfFsTp96LjBnfv3i1JOnLkSD/LR/oOAJC/WkVKK2y71O11u/+yyjDoSKkM7brZpzpT9DIbRJBDp5DUDfaUXsqVmu1hZmam420PIlIqu8NBkKqD+HhmZ2clXR1l5RQpdbqNop1Hiu63qLKGGohICQBQBTRKAIBsjEz6blhT1lcpfZfaR9nb7mXsV07puzCeLYx+b6fdOKRexoGkpCY3bTfmbBDn1vHjxyVd/TiYso89JfdxSr2k1FLCmK54nFen+jlTiEjfAQCqoPYP+cshEsSSEDHEv5dUZ5SyrxzLFqLtl73sZc114eqfc25loVt30VkmRkU4Z+LzvpcIsl1kXkTq+9jNo4I6RaQEAMgGjRIAIBu1T98NWxVTOYMucxXrKIgn8A3pu352FCmqynXaT2WktQYldAbK4XcZOsn00nGiKCIlAEA2iJSQtUF0Fy5LeDhiHD0hL7mfT7l29ulXp4YUIiUAQDZolAAA2aj9jA51k/sTL3NCXXWG+iqOuiqOGR0AAJVFowQAyAaNEgAgGzRKAIBs0CgBALJBowQAyAaNEgAgGzRKAIBs0CgBALJBowQAyAaNEgAgGzRKAIBs0CgBALJBowQAyEZPjZKZvcTMjpvZD8zsaTP7bTO70cweN7Mzjf9fWlZhAQD11muk9GFJf+Huvy7ptyQ9LekBSSfcfUrSicbPAAC01fVD/szsxZK+LenlHm3EzJ6RtNndL5jZrZK+6O6vaLMtno4FAKOjLw/5e7mkS5KOmtm3zOyImb1I0i3ufkGSGv/f3MM+AAAjpJdGaZWkV0v6U3d/laS/UwepOjO738xOm9npHsoAAKiRXhqlOUlz7v61xs/HtdhIPd9I26nx/8XUh939kLvfuVIIBwAYPV03Su7+E0nnzSzcL7pL0vclPSZpe2PddkmP9lRCAMDIWNXj5/+dpE+Y2S9J+pGknVps6B42s3dKelbS1m423G0HjLozs2XrqKs06qoz1Fdx1FVxqbpq+f4cKjLV+y6HcuWIL0Nx1FVnqK/iqKviVmiU+tL7DgCAUtEoAQCyQaMEAMgGjRIAIBs0SgCAbNAoAQCyQaMEAMgGjRIAIBu9zugAoASbNm1qLp86dWrZ62fPnpUkTUxMDKpIlZIaoHn06NHm8o4dOwZYmuo4fvy4JOnKlSvLXluzZk1zecuWLQMrE5ESACAbNEoAgGyQvkMpik66WMX5wYoe2/nz5yVJ4+PjPW03pOjOnTtXaDujLFWHq1Yt/lnbuXNnc91TTz0lSTpw4MBgCpaJOG05MzPT9XYG+b0lUgIAZINIaQSEq6V2V0pjY2OSpEuXLhXabrhJGgs35CVpcnKyYAnzFq4S20VM69evl3R1HRTtmHD58uXmcrjB3OmU/7lKXa2HaEaSXnjhhY62t3fv3pavd7q9XIXOB2vXru34syFan5uba64LnRWOHTvW8rPT09OSro40B4lICQCQDRolAEA2SN8VEIfPqf78e/bskZTvTdRdu3ZJkjZv3txclwrN5+fnJUlHjhxZ9tmUrVuXHiocUn+pdFUdx4iEMTDxsYV0W5y2TN0gjustiMeE1E0qDbmwsLDs9aI30w8ePFhOwTIXzolUSjxVV3HHmFTqvF3aLhdESgCAbNT+cejhKiy+oo1Hehf5bKpcqRH4g6jLsh7DnNrO4cOHJUm7d+9urguRT3y1ltpGKlrspe7LUPYjq+PtpSKlovsNr8cdSkKk2cl2ytav/cVRebiJHm83ZCLiLMTGjRslSSdPnmxZvpQqfQ+73W+72SrKKF+8jV5mFOFx6ACAyqJRAgBkY2Q6OoS0gSR94AMfkJQeeR+HmmEsRWrcQyqtEE+kGdIPVRI6NcRjSELqJb4xner8kOrkEW7UxnU/yPTdsKRSdanODamUXR21SxvFY7SCUG+p1E+c9ly3bl2PpUNuiJQAANkYmUhp//79zeUw8j51szXW6cjwuPNDDh1IuhXfOA2R0urVq5vrtm/fXmg79913n6Te5tyqujCKPjX7RTt1nB2jqBBRx50kwneqjkMMylbl+ROJlAAA2aBRAgBko5bpu9SEjfG6ffv2SUrP1FDltFs/pCYjLZqOq8uEor0Io+jjuhi1cyw+X4qm3sL7io7BGTVFO1KdOXOmzyUpH5ESACAbtYyU4rmx4q7b14pHkLd6Xx2Fm+hFb6Cnujlv2LCh/IJVTNEIaNSiI/RX0e9eWd/RVnPulY1ICQCQDRolAEA2apW+S90ATd0QJJXSm6L1F8aaxDM6YLSV8d0bte9vu7FGqUd/pCZQrgoiJQBANmoVKQVVvkoA6oyouTriGW3m5uYGtl8iJQBANmiUAADZqHz6LtW5oZunIwK9CmM5SB/3FzM6DEb8CJtB/k0lUgIAZINGCQCQjcqn72LDHr9Ql7RNKj0y7LodJaGH2vz8/HALMkSpsTdYUudejERKAIBs9BQpmdl7Je2S5JK+K2mnpBskfVrShKRzkt7q7j/rqZTpfUuS1qxZs+y18GRZaal/PVdcV4tvYl4rNfkqrq6z+ImoZUtte3x8vG/7y0XRx1qM2nc51cmgn+ffsHUdKZnZbZL+QNKd7n6HpOskbZP0gKQT7j4l6UTjZwAA2uo1fbdK0q+Y2SotRkjPSbpXUniq14ykt/S4DwDAiOg6fefuPzazA5KelfT/JH3B3b9gZre4+4XGey6Y2c0llTXpvvvuW7buiSeeaC7HqTws6Vc6aPXq1c3lhYWFvuxjWOI6S02SWVZHl1FLTwWPPPLIsIuQvVE4N3pJ371Ui1HRpKRflfQiM3t7B5+/38xOm9npbssAAKiXXjo6/K6ks+5+SZLM7DOSfkfS82Z2ayNKulXSxdSH3f2QpEONz3bd/M/MzDSXw035m266qdvNXWXUbvKHm6dxd9OxsbGOtnH48OFl64rewM5dHAmFc2PUnljcq3COhceaxOInQaciglH7Po6qXu4pPSvpdWZ2gy2eLXdJelrSY5K2N96zXdKjvRURADAqermn9DUzOy7pm5IWJH1Li5HPGkkPm9k7tdhwbS2joACA+rMcbpyl0ndFy7Vp06bm8qlTp5a9Hp48WzTNUrT/fyr9MAhlz7awdu3a5nJIn2zYsKG57syZM11ve9iYmaIz/aqv+DsVUsPx9yeMNdy6den6NbXfvXv3SpIOHjzYXBfO1UGfp5xbxa2Qdn3S3e9MvcCMDgCAbFQ+Uho1XKEVR111ZhD1dfz4cUlXR0VBHD3l3jmGc6s4IiUAQGXRKAEAskH6rmJIGxRHXXWG+iqOuiqO9B0AoLJolAAA2aBRAgBkg0YJAJANGiUAQDZolAAA2aBRAgBkg0YJAJANGiUAQDZolAAA2aBRAgBkg0YJAJANGiUAQDZolAAA2Vg17AKsZIXpzpFAXRVHXXWG+iqOuioHkRIAIBs0SgCAbNAoAQCyQaMEAMhGth0dcn3efY43M6mr4qirzuRYX9RVcbnWVStESgCAbGQbKeUqXA2dO3euuW5iYmJg+6/Slc+wrxypq85Upb6oq+KqWFdESgCAbNAoAQCyQfquS5OTk83lHEJkAKgDIiUAQDaIlDq0c+fOYRcBIyw+/17/+tdLknbt2jWs4gClI1ICAGSDRgkAkA3Sd0CFTE9PL1tH+k5at25dc/nSpUtDLEk1xWOJht1xi0gJAJANIqUODfsqoi7iLvVhdgzqtjPU15L5+fnmcjifBjnTCspDpAQAyAaNEgAgGzRKAIBs0CgBALJBR4cOVWXKetQf5yLqqG2kZGYfN7OLZvZUtO5GM3vczM40/n9p9NqDZjZrZs+Y2Rv7VXAAQP0USd9NS7r7mnUPSDrh7lOSTjR+lpndLmmbpFc2PvMRM7uutNICAGqtbaPk7l+W9NNrVt8raaaxPCPpLdH6T7n7z939rKRZSa8pqawAgJrrtqPDLe5+QZIa/9/cWH+bpPPR++Ya65Yxs/vN7LSZne6yDACAmim7o0Pqzmty2Lm7H5J0SJLMrDJD0xlFX46zZ882l7lh3x3ORdRRt5HS82Z2qyQ1/r/YWD8naX30vnFJz3VfPADAKOm2UXpM0vbG8nZJj0brt5nZ9WY2KWlK0td7KyIAYFS0Td+Z2SclbZY0ZmZzkt4v6Y8kPWxm75T0rKStkuTu3zOzhyV9X9KCpHe5+z/2qewAgJpp2yi5+9tWeOmuFd7/QUkf7KVQAIDRxIwOHeKmPHLBuYg6Yu47AEA2aJQAANkgfYeuxemjsbExSVePP1qzZs3Ay4RqCE+HlZaeQrxr167musOHDw+6SMgEkRIAIBtESte4cuVKczl1pc8o+iUTExPN5XDlu3bt2mXv27JlS3P52LFjK25vfn6+uRwir7qIo8rz5xdn4hofH+9pm1U+F+NzJxxHXEdHjhyRdHXkHX8GS8J3b9WqpT/nvZ5bw0SkBADIBo0SACAbpO+uEaefNm7cKEk6efLksIqTtTi1EszNzTWX3/ve90qSjh8/3lwXUjSpVMzevXuby9PT0yWVMj/r1y9ODxmnWEJKb5TF6cgDBw5IWuoEIS3VF3Ul7dixo7k8MzOz8hsje/bskbRUt7kiUgIAZMNyuFmaenTFsMo1OzvbXJ6ampJ09Y360PkhvpIfRFlbjd7P4XdYVLhK27dvX8v39XJMuddVuDEdRwGx0B067iIdxMcWrpaPHj3adVnazQox7PqKOx6lOtGEutq9e3dzXYjgy+4YkXtdxUJ24sMf/nBz3alTpwp9tozjKDDbyJPufmfqBSIlAEA2aJQAANkgfdfC1q1bJV19oz5l1NJ3qTRA6BTSjZ07d0oqLyWaU10V1c3kqqOQvksJ54vUukNM2WWvYl21k/ruDeK8Euk7AEAVjHSX8IWFheZyPBo6SM0+wOMCpM9+9rOSpIMHDzbX9dJdt4pXmGVrVwebNm2SdHWUOqr1Fl/Bh2W+l90J9TfojlutECkBALJBowQAyMZIpu9S40RCqi4ek9RK0ffVURhrdMcddzTXhRumcRpl2GmAYYnPq9SsF90Is4qQpkLdESkBALIx0l3C45t74Uq/3X7DlWo8H14v3aGLyr2bcxh5nxp1X7R8ZUVZw66r1P43bNjQXD5z5kwp2x7VLuEpqeOgS3hxqW72/foONtAlHACQPxolAEA2Rjp9d00Zlq1LlSG8L4fyBTn8DlM6TanUJX0XS42YT+kmxVnF9F3YXzwGsIxOQ6Tv0rMydKOMv3Gk7wAAtUCk1BAeMBfPUhBmJ4gfxkak1LmiV5h1jJRSwoMQw8P+VtIqUpek7du3S+rtgYiDvvpft26dJGl+fr7l+y5duiRJGhsbK7RdIqX25Sv6SA8iJQAAGmiUAADZIH13jVTYGYe7YTYI0nfdCROKhglGV1Ln9F1Ku/MupF7KnjEjh5RUq3RmPFFyqIM4nR7EE9WGc2vU0ncp4fE7UvoRPKmJlEnfAQDQQKTUQk5X3DmVpWxl36Suel3Nzs5Kkqamplq+ry6RUivtynf48GFJ0rZt25rrwqwiREppqZlsUlKRelFESgCAWqBRAgBkg/RdAWEMk7Q0jon0Xb6oq+KqmJI6cuRIc3n37t0rvi+e6aKXGQ6CKtZVN0JK76abbmquC4+rKYr0HQCgFkbyIX+d6vQqAUD/7Nq1a9kyDz8sTy9zKZaBSAkAkA0aJQBANkjfdYhUHpCfuJMBqbxqI1ICAGSDSAlArdSla/aoahspmdnHzeyimT0VrdtvZj8ws++Y2Z+b2Uui1x40s1kze8bM3tivggMA6qdI+m5a0t3XrHtc0h3u/puSfijpQUkys9slbZP0ysZnPmJm15VWWgBArbVN37n7l81s4pp1X4h+/KqkLY3leyV9yt1/Lumsmc1Keo2kJzotGDcri6OuiqOuOkN9FUddlaOMjg7vkPT5xvJtks5Hr8011i1jZveb2WkzO11CGQAANdBTRwcze0jSgqRPhFWJtyXvOrr7IUmHGtvhziQAoPtGycy2S3qzpLt8qbvLnKT48ZHjkp7rvngAgFHSVfrOzO6W9D5J97j730cvPSZpm5ldb2aTkqYkfb33YgIARkHbSMnMPilps6QxM5uT9H4t9ra7XtLjjZt7X3X3f+Pu3zOzhyV9X4tpvXe5+z92U7BBjDWoy41J6qo46qoz1Fdx1FU5Rvp5SnX5BVNXxVFXnaG+iqOuOrLi85RGekaHHBrkTg3rpKSuiqtiXUnUVyeoq+I6rSvmvgMAZINGCQCQDRolAEA2aJQAANmgUQIAZINGCQCQDRolAEA2aJQAANmgUQIAZGOkZ3RA/lKjwas4qn2YQh1OTEw01509e3ZIpckP51h7c3NzkqT16xM5EgUAAAq8SURBVJceAtGvOiJSAgBkg0YJAJAN0ndDNjk5KYl0yrVSKZXLly8PoSTVlKq/kydPDqEk+UrVEd/DtPB3ahCIlAAA2SBSGqD5+XlJ0rp164Zckjy1i47WrFkzyOJURqredu3a1Vw+fPjwIIuTpenpaUnSzp07l71Gp4a0rVu3NpcXFhYkDaauiJQAANmgUQIAZGOkH4c+aGWMh6jjmIp+HVMd62rTpk3N5VOnTkm6Oq0ZbtSPjY11vO061lcQjm3Pnj3NdQcOHOh5e7G61FUQH2M4r+Kxbt1sJ7Li49CJlAAA2SBS6rOyr6jqcoUWjwwPo8XLPo6q11W4uSxJq1evXvZ6uOrv5Yo/VvX6ClKzDwRlHU9d6iolHNuqVUv94F544YWet3cNIiUAQP5olAAA2WCc0gB1c5MwWCEErpzUcRw9erTv+6iSMI4tjGuLDSLFWXX9StvVsa6CvXv3LlvXh5RdIURKAIBs0NGhRDt27Gguz8zMLHu97A4OZWy3n44cOSJJ2r1797LX+jlTQ1XqKp5P7Ny5c8tejyPI+NwqU7sr2pzqq5VBdDyoS12lxMcW5kjcuHFjKdtbAR0dAAD5o1ECAGSD9F0JUqFqSLeUdRO/KimpeLLZcKN+w4YNzXVnzpzpexmqUldr165tLl+5cmXZ64Moa9VTUoP8XVe9rlJSnWoG1CmE9B0AIH9ESiVoNYL80qVLzeVu5iMLqnL1H5dz//79kqR9+/Y11w376j+nuorFXXJDJ5lUl/DYIK5oc62va4XvoLTUgSSeESM4f/58c3l8fLyjfdSlruJONaGuhjDUgEgJAJA/GiUAQDZI35UoTiGkUnmh338YB9CJKqakWj1pt59lrmJdpcTpp3BuxWObgnimkPCIgaLqkpJKidOfrZ72fOzYsebyli1bVnxfXeoqhzFdIn0HAKgCIqU+m56ebi7v3Llz2etFj7MuV/+DOI661FU74dzq13nVyXaqKFV/YfhCauhCHeuKLuEAALRAowQAyAbpuwHq5cZ/nVNSRae5DymVeIaITrdX9bpqp9Ob2FVMSbVLiZch1fmhinVV1NTUVHN5dnZWUv8mkG4gfQcAyF/lI6XUlP9VErr4xo8mSM2XN6pX/6tXr24uhy7Scbfn1IMTR7WuYkUjprKv/k+dOtVc3rRpU0ef7Ubq999pt/ii6hwpxVKdHzp9nEVfIyUz+7iZXTSzpxKv7TUzN7OxaN2DZjZrZs+Y2RvbbR8AgKBI+m5a0t3XrjSz9ZLeIOnZaN3tkrZJemXjMx8xs+tKKSkAoPYKpe/MbELS59z9jmjdcUkfkPSopDvdfd7MHpQkd/8vjff8paT/5O5PtNl+1+m7Xp4Fn5N2E7eSkpK2bt0qSXrta1/bXPfoo49KunqWDOrqaqE+BpG+S81CEUul26piVNJ3QbtUbPhdptKlA+/oYGb3SPqxu3/7mpduk3Q++nmusS61jfvN7LSZne6mDACA+lnV6QfM7AZJD0n6V6mXE+uSlw/ufkjSocY2u77EqNvVyaCEDiKpudRyd/z48b5tuy6R97CsWrX0J6XKUdEgxJ2bgl46qHTzvrh7/bXiTg3hM/H2+tXJrONGSdKvSZqU9O1GAcclfdPMXqPFyCieiXRc0nO9FhIAMBo6Tt+5+3fd/WZ3n3D3CS02RK92959IekzSNjO73swmJU1J+nqpJQYA1Fbbjg5m9klJmyWNSXpe0vvd/WPR6+fU6OjQ+PkhSe+QtCDpPe7++baFGJEZHXrBzfviqKviRu3mfS/Krqu6pIq76UCjFh0dKj94dlTwh7Y46qo4GqXiaJTSym6UurmnBADoEQ1+GnPfAQCyQaMEAMgGjRIAIBs0SgCAbNAoAQCyQaMEAMgGjRIAIBs0SgCAbNAoAQCyQaMEAMgGjRIAIBs0SgCAbNAoAQCyQaMEAMgGjRIAIBs0SgCAbNAoAQCyke2TZ+vyqOBBoK6Ko646Q30VR12Vg0gJAJANGiUAQDZolAAA2aBRAgBkI5eODvOS/kbSWGO56jiOvHAceeE48jKM4/hnK71g7j7IgrRkZqfd/c5hl6NXHEdeOI68cBx5ye04SN8BALJBowQAyEZujdKhYRegJBxHXjiOvHAcecnqOLK6pwQAGG25RUoAgBGWRaNkZneb2TNmNmtmDwy7PEWZ2Xoz+59m9rSZfc/M3t1Yf6OZPW5mZxr/v3TYZS3CzK4zs2+Z2ecaP1fuOMzsJWZ23Mx+0Pi9/HZFj+O9jXPqKTP7pJn9chWOw8w+bmYXzeypaN2K5TazBxvf+2fM7I3DKXXaCseyv3FufcfM/tzMXhK9luWxpI4jem2vmbmZjUXrhnocQ2+UzOw6Sf9N0u9Jul3S28zs9uGWqrAFSXvc/TckvU7Suxplf0DSCXefknSi8XMVvFvS09HPVTyOD0v6C3f/dUm/pcXjqdRxmNltkv5A0p3ufoek6yRtUzWOY1rS3desS5a78V3ZJumVjc98pPH3IBfTWn4sj0u6w91/U9IPJT0oZX8s01p+HDKz9ZLeIOnZaN3Qj2PojZKk10iadfcfufs/SPqUpHuHXKZC3P2Cu3+zsXxZi38Ab9Ni+Wcab5uR9JbhlLA4MxuX9PuSjkSrK3UcZvZiSf9C0sckyd3/wd3/jyp2HA2rJP2Kma2SdIOk51SB43D3L0v66TWrVyr3vZI+5e4/d/ezkma1+PcgC6ljcfcvuPtC48evShpvLGd7LCv8TiTpQ5L+UFLcsWDox5FDo3SbpPPRz3ONdZViZhOSXiXpa5JucfcL0mLDJenm4ZWssD/R4gn6i2hd1Y7j5ZIuSTraSEMeMbMXqWLH4e4/lnRAi1ewFyT9X3f/gip2HJGVyl317/47JH2+sVypYzGzeyT92N2/fc1LQz+OHBql1ENIKtUl0MzWSHpE0nvc/W+HXZ5OmdmbJV109yeHXZYerZL0akl/6u6vkvR3yjPF1VLjnsu9kiYl/aqkF5nZ24dbqr6o7HffzB7SYvr+E2FV4m1ZHouZ3SDpIUn/MfVyYt1AjyOHRmlO0vro53EtpioqwcxWa7FB+oS7f6ax+nkzu7Xx+q2SLg6rfAW9XtI9ZnZOi+nTf2lmf6bqHcecpDl3/1rj5+NabKSqdhy/K+msu19y9xckfUbS76h6xxGsVO5KfvfNbLukN0v61740pqZKx/JrWrzg+XbjOz8u6Ztm9jJlcBw5NErfkDRlZpNm9ktavMn22JDLVIiZmRbvXzzt7n8cvfSYpO2N5e2SHh102Trh7g+6+7i7T2ix/v/K3d+u6h3HTySdN7NXNFbdJen7qthxaDFt9zozu6Fxjt2lxfuVVTuOYKVyPyZpm5ldb2aTkqYkfX0I5SvMzO6W9D5J97j730cvVeZY3P277n6zu080vvNzkl7d+P4M/zjcfej/JL1Jiz1Z/lrSQ8MuTwfl3qjF0PY7kv5X49+bJN2kxV5GZxr/3zjssnZwTJslfa6xXLnjkPTPJZ1u/E4+K+mlFT2O/yzpB5KekvQ/JF1fheOQ9Ekt3gd7QYt/7N7ZqtxaTCP9taRnJP3esMtf4FhmtXjPJXzfP5r7saSO45rXz0kay+U4mNEBAJCNHNJ3AABIolECAGSERgkAkA0aJQBANmiUAADZoFECAGSDRgkAkA0aJQBANv4/V6n93ek6iicAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1152x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_sample(s_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from tqdm import tnrange\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PrototypeModel(3, 64,64)\n",
    "model = model.to(ctx)\n",
    "dataloader = DataLoader(data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Challenger\\Anaconda3\\envs\\epytorch\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PrototypeModel(\n",
       "  (conv_blocks): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.001)\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.001)\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_size = 100\n",
    "epoch = 0\n",
    "loss = 0\n",
    "accuracy = 0\n",
    "lr = 0.1\n",
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "#divide the learning rate by 2 at each epoch, as suggested in paper\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.5, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Challenger\\Anaconda3\\envs\\epytorch\\lib\\site-packages\\ipykernel_launcher.py:4: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639fccf2b5cd4701a200fa8880e40857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 train:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Challenger\\Anaconda3\\envs\\epytorch\\lib\\site-packages\\ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1630e38a5dce4ba59cf87dcd23d30b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mean loss: 0.052, mean accuracy: 0.988\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2a9b1ea16944b192d02af75a293ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mean loss: 0.037, mean accuracy: 0.991\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912ce8c6b4d14602878eca8580cf2d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mean loss: 0.03, mean accuracy: 0.993\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a062987572144c7b8ad57743753f2f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mean loss: 0.026, mean accuracy: 0.994\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14cd5701d8cd476087c042d1b3c2145f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mean loss: 0.023, mean accuracy: 0.995\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08bc3226fd0645be892b65905c034f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mean loss: 0.022, mean accuracy: 0.995\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501974e4a8f54cfa945c0d604fbcb7a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mean loss: 0.021, mean accuracy: 0.995\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f7d25d3e3f43fcaf597b71d4d144b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mean loss: 0.02, mean accuracy: 0.995\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ad98406a8c47bea1dc0631ac60726a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mean loss: 0.019, mean accuracy: 0.996\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc36443873854a258048eb28520f5f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-8f14070a5a07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0maccuracies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\epytorch\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\epytorch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# traininig \n",
    "accuracies = []\n",
    "losses = []\n",
    "for episode in tnrange(epoch_size, desc=\"Epoch {:d} train\".format(epoch+1)):\n",
    "    \n",
    "    for (s_in, b_in) in tqdm_notebook(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        loss,acc,_ = get_loss(model, s_in.squeeze(0).to(ctx), b_in.squeeze(0).to(ctx))\n",
    "        accuracies.append(acc.item())\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\" mean loss: {round(np.mean(losses),3)}, mean accuracy: {round(np.mean(accuracies),3)}\")\n",
    "    scheduler.step()\n",
    "        \n",
    "#         print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classes = np.unique(testy)\n",
    "data_val = Alphabet(testx, testy, test_classes, n_class_per_batch=5, epoch_size=2000, my_transforms=test_transforms, test=False)\n",
    "testloader = DataLoader(data_val, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  for idx, (s_in, b_in) in enumerate(testloader):\n",
    "#     print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Challenger\\Anaconda3\\envs\\epytorch\\lib\\site-packages\\ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e1abbce9f242769953f120cefd4123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mean loss: 0.022, mean accuracy: 0.995\n"
     ]
    }
   ],
   "source": [
    "# traininig \n",
    "accuracies = []\n",
    "losses = []\n",
    "preds = []\n",
    "for (s_in, b_in) in tqdm_notebook(testloader):\n",
    "    with torch.no_grad():\n",
    "        loss,acc,y_hat = get_loss(model, s_in.squeeze(0).to(ctx), b_in.squeeze(0).to(ctx))\n",
    "        preds.append(y_hat)\n",
    "    accuracies.append(acc.item())\n",
    "    losses.append(loss.item())\n",
    "\n",
    "print(f\" mean loss: {round(np.mean(losses),3)}, mean accuracy: {round(np.mean(accuracies),3)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
